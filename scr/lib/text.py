def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –ø—É—Ç–µ–º —É–¥–∞–ª–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É.
    
    –§—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:
    - –£–¥–∞–ª—è–µ—Ç —Å–∏–º–≤–æ–ª—ã —Ç–∞–±—É–ª—è—Ü–∏–∏ (\t) –∏ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç—Ä–æ–∫–∏ (\n)
    - –£–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã (–≤ –Ω–∞—á–∞–ª–µ, –∫–æ–Ω—Ü–µ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏)
    - –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º casefold()
    - –ó–∞–º–µ–Ω—è–µ—Ç –±—É–∫–≤—É '—ë' –Ω–∞ '–µ' (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """

    if not isinstance(text, str):
        raise ValueError("normalize: text –Ω–µ str")

    result = (((text.replace("\t"," ")).replace("\r"," ")).replace("\n"," "))
    result = " ".join((result.strip()).split())

    if casefold:
        result = result.casefold()

    if yo2e:
        result = result.replace('—ë', '–µ')

    return result

try:
    print(f"\nnormalize")
    print("–¢–µ—Å—Ç 1:", normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
    print("–¢–µ—Å—Ç 2:", normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))
    print("–¢–µ—Å—Ç 3:", normalize("Hello\r\nWorld"))
    print("–¢–µ—Å—Ç 3:", normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))

except ValueError as e:
    print(f"–û—à–∏–±–∫–∞! {e}")


def tokenize(text: str) -> list[str]:
    """
    –§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –≤—Ö–æ–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É –Ω–∞ —á–∞—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
    –ª—é–±—ã–µ —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –±—É–∫–≤–∞–º–∏ –∏–ª–∏ —Ü–∏—Ñ—Ä–∞–º–∏.
    """
    import re

    if not isinstance(text, str):
        raise ValueError("tokenize: text –Ω–µ str")
    
    split_result = re.split("[^\w-]+", text)
    
    return [item for item in split_result if len(item) >= 1]
    
try:
    print(f"\ntokenize")
    print("–¢–µ—Å—Ç 1:", tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
    print("–¢–µ—Å—Ç 2:", tokenize("hello,world!!!"))
    print("–¢–µ—Å—Ç 3:", tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
    print("–¢–µ—Å—Ç 4:", tokenize("2025 –≥–æ–¥"))
    print("–¢–µ—Å—Ç 5:", tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))

except ValueError as e:
    print(f"–û—à–∏–±–∫–∞! {e}")


def count_freq(tokens: list[str]) -> dict[str, int]:
    """
    AAAAAAAAAA
    """
    from collections import Counter

    if not isinstance(tokens, list):
        raise ValueError("tokenize: text –Ω–µ str")

    return dict(Counter(tokens))

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    """
    AAAAAAAAAAAAA
    """

    if not isinstance(freq, dict):
        raise ValueError("top_n: freq –Ω–µ  dict")
    
    return sorted(freq.items())[:n]

try:
    print(f"\ncount_freq + top_n")
    print("–¢–µ—Å—Ç 1:", top_n(count_freq(["a","b","a","c","b","a"]), 2))
    print("–¢–µ—Å—Ç 2:", top_n(count_freq(["bb","aa","bb","aa","cc"]), 2))

except ValueError as e:
    print(f"–û—à–∏–±–∫–∞! {e}")